{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yarin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "We'll reupload our mailing list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName='/Users/yarin/Downloads/hillary-clinton-emails/mailinglist.csv'\n",
    "with open(fileName) as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remember our mailing list fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'emailId', 'sender', 'senderGender',\n",
       "       'ExtractedSubject', 'ExtractedBodyText', 'receiver',\n",
       "       'receiverGender', 'mailLength', 'spaces'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is the mail sender gender, and our text is inside the ExtractedBodyText field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df[['ExtractedBodyText','senderGender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we want to remove dates, punctuations, numbers, single letters and stopwords. Also we'll convert all chars to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "dateWords = ['sunday','monday','tuesday','wednesday','thursday','friday','saturday'\n",
    "             ,'january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    row['ExtractedBodyText'] = row['ExtractedBodyText'].lower()\n",
    "    row['ExtractedBodyText'] = row['ExtractedBodyText'].translate(None, punctuation)\n",
    "    row['ExtractedBodyText'] = row['ExtractedBodyText'].translate(None, digits)\n",
    "    row['ExtractedBodyText'] = ' '.join( [w for w in row['ExtractedBodyText'].split() if len(w)>1] )\n",
    "    row['ExtractedBodyText'] = ' '.join([word for word in row['ExtractedBodyText'].split() if word not in stopWords])\n",
    "    row['ExtractedBodyText'] = ' '.join([word for word in row['ExtractedBodyText'].split() if word not in dateWords])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example rows from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>senderGender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hrodclintonemailcom pm huma abedin fw latest s...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pis print • hrodclintonernailcom pm °russorvst...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hrodclintonemailcorn pm huma abedin fw latest ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>randolph lawrence sent pm mills cheryl subject...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asked attend svtc today embassy tripoli first ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazing sullivan jacob sullivaniistategov pm s...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sidney blumenthal magariaf privat reax sent me...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pis print sullivan jacob mailtosullivanjjstate...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>us department state case doc date state dept p...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>us department state case doc date state dept p...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>us department state case doc date state dept p...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>one wall worth dubious arming libyan rebels be...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>romney oped todays wall street journal playing...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>remarks afternoon andrews wonderful thanks muc...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>info sidney blumenthal mailto sent pm subject ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fyi — take care job prom rhodes benjamin sent ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>state dept produced house select benghazi comm...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pouch call sheets schedule en route well also ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fyi —jeff discussed fully agree nato liaison g...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hrodclintonernailcom pm sullivanjjstategovi fv...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ExtractedBodyText senderGender\n",
       "0   hrodclintonemailcom pm huma abedin fw latest s...            f\n",
       "1   pis print • hrodclintonernailcom pm °russorvst...            f\n",
       "2   hrodclintonemailcorn pm huma abedin fw latest ...            f\n",
       "3   randolph lawrence sent pm mills cheryl subject...            f\n",
       "4   asked attend svtc today embassy tripoli first ...            m\n",
       "5   amazing sullivan jacob sullivaniistategov pm s...            m\n",
       "6   sidney blumenthal magariaf privat reax sent me...            f\n",
       "7   pis print sullivan jacob mailtosullivanjjstate...            f\n",
       "8   us department state case doc date state dept p...            f\n",
       "9   us department state case doc date state dept p...            f\n",
       "10  us department state case doc date state dept p...            f\n",
       "11  one wall worth dubious arming libyan rebels be...            f\n",
       "12  romney oped todays wall street journal playing...            f\n",
       "13  remarks afternoon andrews wonderful thanks muc...            m\n",
       "14  info sidney blumenthal mailto sent pm subject ...            f\n",
       "15  fyi — take care job prom rhodes benjamin sent ...            m\n",
       "16  state dept produced house select benghazi comm...            f\n",
       "17  pouch call sheets schedule en route well also ...            f\n",
       "18  fyi —jeff discussed fully agree nato liaison g...            m\n",
       "19  hrodclintonernailcom pm sullivanjjstategovi fv...            f"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we attempted to use the following text vectorizers, although the normal count vectorizer was the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None,stop_words = None,max_features = 10000)\n",
    "vectorizer3 = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None,ngram_range=(1,3),stop_words = None,max_features = 10000)\n",
    "vectorizer2 = TfidfVectorizer(analyzer = \"word\", tokenizer = None,lowercase=True,preprocessor = None,ngram_range=(1,3),stop_words = None,max_features = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(df2['ExtractedBodyText'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2266, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features = train_data_features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'aa',\n",
       " u'aab',\n",
       " u'aar',\n",
       " u'abandon',\n",
       " u'abandoned',\n",
       " u'abbas',\n",
       " u'abbreviated',\n",
       " u'abdominal',\n",
       " u'abduction',\n",
       " u'abdullah',\n",
       " u'abed',\n",
       " u'abedin',\n",
       " u'abedinflstategov',\n",
       " u'abedinh',\n",
       " u'abedinhstategov',\n",
       " u'abedinhstategoy',\n",
       " u'ability',\n",
       " u'able',\n",
       " u'aboard',\n",
       " u'abolish',\n",
       " u'abolished',\n",
       " u'aboul',\n",
       " u'aboulgheit',\n",
       " u'aboulgheits',\n",
       " u'abraham',\n",
       " u'abroad',\n",
       " u'abruptly',\n",
       " u'absentee',\n",
       " u'absolutely',\n",
       " u'absorb',\n",
       " u'abstain',\n",
       " u'abstention',\n",
       " u'abu',\n",
       " u'abuses',\n",
       " u'abushakour',\n",
       " u'abyei',\n",
       " u'abz',\n",
       " u'academic',\n",
       " u'academics',\n",
       " u'accelerating',\n",
       " u'accept',\n",
       " u'acceptable',\n",
       " u'acceptance',\n",
       " u'accepted',\n",
       " u'accepting',\n",
       " u'access',\n",
       " u'accessibility',\n",
       " u'accidentally',\n",
       " u'accion',\n",
       " u'acclaimed']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we split our dataset into 70% train set and 30% test set.\n",
    "we ran random forest, svm and knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835755813953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(123)\n",
    "x = np.random.rand(len(df2['ExtractedBodyText'])) < 0.7\n",
    "train_x = train_data_features[x]\n",
    "test_x = train_data_features[~x]\n",
    "train_y = df2.loc[x,\"senderGender\"]\n",
    "test_y = df2.loc[~x,\"senderGender\"]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "model = forest.fit( train_x, train_y )\n",
    "rf = model.score(test_x,test_y)\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822674418605\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "clf = svm.LinearSVC() \n",
    "clf.fit(train_x, train_y) \n",
    "svm = clf.score(test_x,test_y)\n",
    "print(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747093023256\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors \n",
    "knn = neighbors.KNeighborsClassifier(3) \n",
    "knn.fit(train_x, train_y) \n",
    "knns = knn.score(test_x,test_y)\n",
    "print(knns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x144292d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models=[knns,svm,rf]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Final Scores')\n",
    "index = np.arange(3)\n",
    "plt.bar(index,models,alpha=0.8, color='b',label='Model')\n",
    "ax.set_xticklabels(('','','KNN','','SVM','','RandomForest'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest prevails with 83.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 2266)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "##path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "#text = open(path).read()\n",
    "print('corpus length:', len(df2['ExtractedBodyText']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split our dataset into two sparate datasets, and train a simple neural neteork, using the embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "females=df2[df2.senderGender == 'f']\n",
    "males=df2[df2.senderGender == 'm']\n",
    "\n",
    "females = females[\"ExtractedBodyText\"]\n",
    "males = males[\"ExtractedBodyText\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll append all mail texts into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "females = females.str.cat(sep=' ')\n",
    "males = males.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example text from the females dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produced house select benghazi comm subject agreement sensitive information redactions foia waiver statecb hrodclintonemailcorn pm huma abedin fw latest syria aiding qaddafi sid hrc memo syria aiding libya docx pis print randolph lawrence sent pm mills cheryl subject dry eye nea including mine remarks really moving chriswas amazing man huge loss know libya coming almost ten years never worked calm cool headed funny diplomat made seem really easy even one hardest places work world sidney blumenth'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females[500:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize both of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "females = text_to_word_sequence(females, lower=False, split=\" \")\n",
    "males = text_to_word_sequence(males, lower=False, split=\" \")\n",
    "\n",
    "fToken = Tokenizer(nb_words=1000,char_level=False)\n",
    "mToken = Tokenizer(nb_words=1000,char_level=False)\n",
    "fToken.fit_on_texts(females)\n",
    "mToken.fit_on_texts(males)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create two term matrices for each of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18149, 1000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_mtx = fToken.texts_to_matrix(females, mode='binary')\n",
    "mas_mtx = mToken.texts_to_matrix(males, mode='binary')\n",
    "fem_mtx.shape\n",
    "mas_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51802, 1000), (51802, 1000))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fem = fem_mtx[:-1]\n",
    "output_fem = fem_mtx[1:]\n",
    "input_fem.shape, output_fem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18148, 1000), (18148, 1000))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mas = mas_mtx[:-1]\n",
    "output_mas = mas_mtx[1:]\n",
    "input_mas.shape, output_mas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create neural networks using the embedding method with 42 embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelF = Sequential()\n",
    "modelM = Sequential()\n",
    "\n",
    "modelF.add(Embedding(input_dim=input_fem.shape[1],output_dim= 42, input_length=input_fem.shape[1]))\n",
    "modelM.add(Embedding(input_dim=input_mas.shape[1],output_dim= 42, input_length=input_mas.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read that using softmax as an activation function, is more suitible to text mining jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelF.add(Flatten())\n",
    "modelM.add(Flatten())\n",
    "\n",
    "modelF.add(Dense(output_fem.shape[1], activation='softmax'))\n",
    "modelM.add(Dense(output_mas.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our two models with batch size 300 and run it for 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41441 samples, validate on 10361 samples\n",
      "Epoch 1/10\n",
      "41441/41441 [==============================] - 186s - loss: 4.4981 - acc: 0.0219 - val_loss: 4.4045 - val_acc: 0.0236\n",
      "Epoch 2/10\n",
      "41441/41441 [==============================] - 187s - loss: 4.2476 - acc: 0.0511 - val_loss: 4.2659 - val_acc: 0.0504\n",
      "Epoch 3/10\n",
      "41441/41441 [==============================] - 185s - loss: 3.9360 - acc: 0.0892 - val_loss: 4.0791 - val_acc: 0.0734\n",
      "Epoch 4/10\n",
      "41441/41441 [==============================] - 186s - loss: 3.6459 - acc: 0.1177 - val_loss: 4.0752 - val_acc: 0.0858\n",
      "Epoch 5/10\n",
      "41441/41441 [==============================] - 177s - loss: 3.4224 - acc: 0.1336 - val_loss: 4.0021 - val_acc: 0.0855\n",
      "Epoch 6/10\n",
      "41441/41441 [==============================] - 178s - loss: 3.2742 - acc: 0.1418 - val_loss: 4.0268 - val_acc: 0.0841\n",
      "Epoch 7/10\n",
      "41441/41441 [==============================] - 178s - loss: 3.1767 - acc: 0.1457 - val_loss: 4.1884 - val_acc: 0.0803\n",
      "Epoch 8/10\n",
      "41441/41441 [==============================] - 177s - loss: 3.1053 - acc: 0.1488 - val_loss: 4.0726 - val_acc: 0.0847\n",
      "Epoch 9/10\n",
      "41441/41441 [==============================] - 177s - loss: 3.0645 - acc: 0.1493 - val_loss: 4.0503 - val_acc: 0.0892\n",
      "Epoch 10/10\n",
      "41441/41441 [==============================] - 178s - loss: 3.0227 - acc: 0.1523 - val_loss: 4.2027 - val_acc: 0.0845\n",
      "Train on 14518 samples, validate on 3630 samples\n",
      "Epoch 1/10\n",
      "14518/14518 [==============================] - 63s - loss: 4.6175 - acc: 0.0042 - val_loss: 4.5737 - val_acc: 0.0030\n",
      "Epoch 2/10\n",
      "14518/14518 [==============================] - 62s - loss: 4.3650 - acc: 0.0070 - val_loss: 4.6003 - val_acc: 0.0052\n",
      "Epoch 3/10\n",
      "14518/14518 [==============================] - 62s - loss: 4.3226 - acc: 0.0132 - val_loss: 4.5532 - val_acc: 0.0110\n",
      "Epoch 4/10\n",
      "14518/14518 [==============================] - 63s - loss: 4.2627 - acc: 0.0237 - val_loss: 4.7687 - val_acc: 0.0124\n",
      "Epoch 5/10\n",
      "14518/14518 [==============================] - 62s - loss: 4.1796 - acc: 0.0365 - val_loss: 4.7070 - val_acc: 0.0179\n",
      "Epoch 6/10\n",
      "14518/14518 [==============================] - 62s - loss: 4.0818 - acc: 0.0487 - val_loss: 4.6380 - val_acc: 0.0237\n",
      "Epoch 7/10\n",
      "14518/14518 [==============================] - 62s - loss: 3.9513 - acc: 0.0647 - val_loss: 4.5432 - val_acc: 0.0251\n",
      "Epoch 8/10\n",
      "14518/14518 [==============================] - 62s - loss: 3.8245 - acc: 0.0791 - val_loss: 4.4657 - val_acc: 0.0388\n",
      "Epoch 9/10\n",
      "14518/14518 [==============================] - 62s - loss: 3.6774 - acc: 0.0946 - val_loss: 4.4726 - val_acc: 0.0408\n",
      "Epoch 10/10\n",
      "14518/14518 [==============================] - 62s - loss: 3.5282 - acc: 0.1084 - val_loss: 4.5319 - val_acc: 0.0394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d423990>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelF.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "modelF.fit(input_fem, y=output_fem, batch_size=300, nb_epoch=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "modelM.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "modelM.fit(input_mas, y=output_mas, batch_size=300, nb_epoch=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built two text generation functions:\n",
    "\n",
    "get_next - is a random term generator for each of the sexes\n",
    "\n",
    "get_next2 - finds the ten best terms and randomly chooses one of them.\n",
    "Unfortunatly we have encountered duplications and looping terms, and had to choose a random term in case of a term loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(text,token,model,fullmtx,fullText):\n",
    "    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n",
    "    tmp = token.sequences_to_matrix(tmp, mode='binary')\n",
    "    p = model.predict(tmp)\n",
    "    next_idx = np.random.random_sample()*1000\n",
    "    return fullText[int(next_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next2(listW,text,token,model,fullmtx,fullText):\n",
    "    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n",
    "    tmp = token.sequences_to_matrix(tmp, mode='binary')\n",
    "    p = model.predict(tmp)\n",
    "    bestMatch = np.min(np.argmax(p))\n",
    "    tmpIdx = np.where(fem_mtx[:,bestMatch]>0)\n",
    "    listW.append(text)\n",
    "    i = 0\n",
    "    result = text\n",
    "    while i<10 and result in listW:\n",
    "        wIdx = int(np.random.random()*10)\n",
    "        next_idx = tmpIdx[0][wIdx]\n",
    "        result = fullText[int(next_idx)]\n",
    "        i= i+1\n",
    "    if result in listW:\n",
    "        next_idx = np.random.random_sample()*1000\n",
    "        result = fullText[int(next_idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tokenize once again for our new texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51803, 8364)\n"
     ]
    }
   ],
   "source": [
    "vectorizerF = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None,stop_words = None,max_features = 10000)\n",
    "vectorizerM = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None,stop_words = None,max_features = 10000)\n",
    "\n",
    "featuresF = vectorizerF.fit_transform(females)\n",
    "featuresM = vectorizerM.fit_transform(males)\n",
    "featuresF = featuresF.toarray()\n",
    "featuresM = featuresM.toarray()\n",
    "vocabF = vectorizerF.get_feature_names()\n",
    "vocabM = vectorizerM.get_feature_names()\n",
    "dist = np.sum(featuresF, axis=0)\n",
    "print(featuresF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = \"\"\n",
    "m = \"\"\n",
    "for i in range(0,len(vocabF)):\n",
    "    f = f + unicodedata.normalize('NFKD', vocabF[i]).encode('ascii','ignore') + \" \"\n",
    "vocabF = f\n",
    "for i in range(0,len(vocabM)):\n",
    "    m = m + unicodedata.normalize('NFKD', vocabM[i]).encode('ascii','ignore') + \" \"\n",
    "vocabM = m  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabF = text_to_word_sequence(vocabF, lower=False, split=\" \")\n",
    "vocabM = text_to_word_sequence(vocabM, lower=False, split=\" \")\n",
    "\n",
    "fToken2 = Tokenizer(nb_words=1000,char_level=False)\n",
    "mToken2 = Tokenizer(nb_words=1000,char_level=False)\n",
    "fToken2.fit_on_texts(vocabF)\n",
    "mToken2.fit_on_texts(vocabM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we move on to generate 400 sequences for each of our sexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordF = \"hello\"\n",
    "wordM = \"hello\"\n",
    "femGenSenList = []\n",
    "masGenSenList = []\n",
    "listMW = []\n",
    "listFW = []\n",
    "for index1 in range(0,400):\n",
    "    f = \"\"\n",
    "    m = \"\"\n",
    "    listMW = []\n",
    "    listFW = []\n",
    "    senlen = np.random.random()*10+10\n",
    "    for index2 in range(0,int(senlen)):\n",
    "        wordF = get_next(wordF,fToken,modelF,fem_mtx,females)\n",
    "        wordM = get_next(wordM,mToken,modelM,mas_mtx,males)\n",
    "        f = f + \" \" + wordF\n",
    "        m = m + \" \" + wordM \n",
    "    femGenSenList.append(f)\n",
    "    masGenSenList.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' droid reasonable us either fabius doc riots waiver adding stevens argument place memo department posted subject memo',\n",
       " ' print sent important department speech egypt fyi foia morocco egypt produced wall books ed faction case remarks',\n",
       " ' hanleymrstategov really stevens reax last waiver standing particularly diplomacy know campaign pis case',\n",
       " ' select nea diplomacy doc also foia speech anytime aq romney operate king waiver one think case rebel cant',\n",
       " ' us send fm tactic doc cant aiding state nwt review',\n",
       " ' ubl select romney department alfaisal memo blumenthal libya diplomacy produced place',\n",
       " ' published thank society york hillary reading benghazi date rove information approve',\n",
       " ' seem subject justices huge offering abushakour annemarie deadly produced security strategists us produced case gotten thank',\n",
       " ' particularly fm yesterday huma sent memo terrific docx date waiver subject house libya pmelect yesterday film ed',\n",
       " ' wall annemarie deadly comm state mailtosullivanjjstategov front saud gillespie hrodclintonemailcom understandably print playing',\n",
       " ' comm dept bogeyman years huge beat york picked sides sent really heard yesterday today right moving argument wrote',\n",
       " ' roves part weekend subject subject well produced state pm us syria help todays make new fvv pis',\n",
       " ' roves security doc violence case stevens department nb know benghazi presidents statescb house wanted ubl',\n",
       " ' pouch select libya know produced hillary us risk dept waiver state heard romney subject something statecb doc',\n",
       " ' case hardest new laden uzi fm pm stevens diplomats know stevens',\n",
       " ' diplomacy fm thought bogeyman us campaign hague huge real playing hours redactions',\n",
       " ' murder real mine state farewell according beat wing sensitive posted perfect subject',\n",
       " ' date comm statescb benghazi true journal rove defense sensitive meet would speak raise',\n",
       " ' sidney called subject know sidney agreement waiver even including later jilotylcstategov hear bin docx wont tactic netanyahu',\n",
       " ' film scheduled reading date subject statecb department dept oped ubl yesterday farewell say last reinforce ubl transitional']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femGenSenList[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' states well tenure secretary redactions janet share friendly subject like sent even wonderful fw reference us hrc',\n",
       " ' date tightening house state definitely diplomats credible dept tooth youve using libyan sherman denigrate matt theory soon',\n",
       " ' floor hours forward case chair state panel locations wonderful statecb pass subject well',\n",
       " ' hit first im seen cc pass jacob ondisturbing comm meaningful president carp wasnt forward nea knee minimize requested',\n",
       " ' see using ive like presence keene select nea space since',\n",
       " ' dont libya conflict agreement clear bb team fwd nation presence minister',\n",
       " ' pm waiver timely gordon minimize wants mtg decision qatar would worth',\n",
       " ' rest think sent department agreement dont interviews mouth sensitive matter violent hillary gene dont weve fw',\n",
       " ' among pass department going space thing information suggested pm diplomats locations already fyi agreement statement gravy president',\n",
       " ' gene cover nuland pls first embassy hrc one team embassy waiver nation waiver',\n",
       " ' case jacob polaschlk misrata doc interviews torso mexico good redactions kind heffern gravy ipad sullivan jacob close bums',\n",
       " ' wendy comm everything agreement sullivan chris course seen seen would could never sent fw info reported victoria',\n",
       " ' sid asked uae department case ive resolve question pass tajordan even produced sullivan insisted leg',\n",
       " ' like ski libya seen conspiracy westerners date scan fully dan house persorg polaschlk already benghazi sullivan latter',\n",
       " ' reported obama gene benghazi state qadhati subject try staff best todays',\n",
       " ' pass ofiuwaili agreement obama back united best invade statement jacob select hrc',\n",
       " ' sid victoria reason intel department latter strange type attack intentional tunadolphin report',\n",
       " ' mexico went redactions nato like greet house depending us wants keene ondisturbing sid',\n",
       " ' sullivan standard rolling jacob like recognition cret access doc state nailed ondisturbing ambassador like appropriate hours wasnt',\n",
       " ' us enough dont jacob nations transcript president fw house iraq afternoon doc ambassador conspiracy evacuate appropriate case']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masGenSenList[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to combine our new generated texts with the old dataset, but first let's combine the two new generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfF = pd.DataFrame(femGenSenList)\n",
    "dfM = pd.DataFrame(masGenSenList)\n",
    "\n",
    "dfF['gender'] = 'f'\n",
    "dfM['gender'] = 'm'\n",
    "\n",
    "dfF.columns = ['ExtractedBodyText','senderGender']\n",
    "dfM.columns = ['ExtractedBodyText','senderGender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileRNN = dfF.append(dfM,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>senderGender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>droid reasonable us either fabius doc riots w...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>print sent important department speech egypt ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hanleymrstategov really stevens reax last wai...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>select nea diplomacy doc also foia speech any...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us send fm tactic doc cant aiding state nwt r...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ubl select romney department alfaisal memo bl...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>published thank society york hillary reading ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>seem subject justices huge offering abushakou...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>particularly fm yesterday huma sent memo terr...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wall annemarie deadly comm state mailtosulliv...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>comm dept bogeyman years huge beat york picke...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>roves part weekend subject subject well produ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>roves security doc violence case stevens depa...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pouch select libya know produced hillary us r...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>case hardest new laden uzi fm pm stevens dipl...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diplomacy fm thought bogeyman us campaign hag...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>murder real mine state farewell according bea...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>date comm statescb benghazi true journal rove...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sidney called subject know sidney agreement w...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>film scheduled reading date subject statecb d...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ExtractedBodyText senderGender\n",
       "0    droid reasonable us either fabius doc riots w...            f\n",
       "1    print sent important department speech egypt ...            f\n",
       "2    hanleymrstategov really stevens reax last wai...            f\n",
       "3    select nea diplomacy doc also foia speech any...            f\n",
       "4    us send fm tactic doc cant aiding state nwt r...            f\n",
       "5    ubl select romney department alfaisal memo bl...            f\n",
       "6    published thank society york hillary reading ...            f\n",
       "7    seem subject justices huge offering abushakou...            f\n",
       "8    particularly fm yesterday huma sent memo terr...            f\n",
       "9    wall annemarie deadly comm state mailtosulliv...            f\n",
       "10   comm dept bogeyman years huge beat york picke...            f\n",
       "11   roves part weekend subject subject well produ...            f\n",
       "12   roves security doc violence case stevens depa...            f\n",
       "13   pouch select libya know produced hillary us r...            f\n",
       "14   case hardest new laden uzi fm pm stevens dipl...            f\n",
       "15   diplomacy fm thought bogeyman us campaign hag...            f\n",
       "16   murder real mine state farewell according bea...            f\n",
       "17   date comm statescb benghazi true journal rove...            f\n",
       "18   sidney called subject know sidney agreement w...            f\n",
       "19   film scheduled reading date subject statecb d...            f"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileRNN[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df2['flag'] = 'old'\n",
    "fileRNN['flag'] = 'new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.append(fileRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(df3['ExtractedBodyText'])\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3066, 10000)\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old=df3[df3.flag == 'old']\n",
    "new=df3[df3.flag == 'new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = new['senderGender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3066, 10000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = train_data_features[2266:3066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 10000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run our old models on the new generated texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52875\n"
     ]
    }
   ],
   "source": [
    "rf2 = model.score(t,new)\n",
    "print(rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm2 = clf.score(t,new)\n",
    "print(svm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51625\n"
     ]
    }
   ],
   "source": [
    "knn2 = knn.score(t,new) \n",
    "print(knn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx78ZpckAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133945950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models=[knn2,svm2,rf2]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Final Scores')\n",
    "index = np.arange(3)\n",
    "plt.bar(index,models,alpha=0.8, color='b',label='Model')\n",
    "ax.set_xticklabels(('','','KNN','','SVM','','RandomForest'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our second text generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordF = \"hello\"\n",
    "wordM = \"hello\"\n",
    "femGenSenList = []\n",
    "masGenSenList = []\n",
    "listMW = []\n",
    "listFW = []\n",
    "for index1 in range(0,400):\n",
    "    f = \"\"\n",
    "    m = \"\"\n",
    "    listMW = []\n",
    "    listFW = []\n",
    "    senlen = np.random.random()*10+10\n",
    "    for index2 in range(0,int(senlen)):\n",
    "        wordF = get_next2(listFW,wordF,fToken,modelF,fem_mtx,females)\n",
    "        wordM = get_next2(listMW,wordM,mToken,modelM,mas_mtx,males)\n",
    "        f = f + \" \" + wordF\n",
    "        m = m + \" \" + wordM \n",
    "    femGenSenList.append(f)\n",
    "    masGenSenList.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go through the same phases as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "dfF = pd.DataFrame(femGenSenList)\n",
    "dfM = pd.DataFrame(masGenSenList)\n",
    "\n",
    "dfF['gender'] = 'f'\n",
    "dfM['gender'] = 'm'\n",
    "\n",
    "dfF.columns = ['ExtractedBodyText','senderGender']\n",
    "dfM.columns = ['ExtractedBodyText','senderGender']\n",
    "\n",
    "fileRNN = dfF.append(dfM,ignore_index=True)\n",
    "df2['flag'] = 'old'\n",
    "fileRNN['flag'] = 'new'\n",
    "df3 = df2.append(fileRNN)\n",
    "train_data_features = vectorizer.fit_transform(df3['ExtractedBodyText'])\n",
    "train_data_features = train_data_features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "old=df3[df3.flag == 'old']\n",
    "new=df3[df3.flag == 'new']\n",
    "new = new['senderGender']\n",
    "t = train_data_features[2266:3066]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50375\n"
     ]
    }
   ],
   "source": [
    "rf3 = model.score(t,new)\n",
    "print(rf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52625\n"
     ]
    }
   ],
   "source": [
    "svm3 = clf.score(t,new)\n",
    "print(svm3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n"
     ]
    }
   ],
   "source": [
    "knn3 = knn.score(t,new) \n",
    "print(knn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/X+vYmiTYTPNgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1445ef750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models=[knn3,svm3,rf3]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Final Scores')\n",
    "index = np.arange(3)\n",
    "plt.bar(index,models,alpha=0.8, color='b',label='Model')\n",
    "ax.set_xticklabels(('','','KNN','','SVM','','RandomForest'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found it very exciting and difficult to train a neural network for text mining purposes.\n",
    "Our svm random text generator preformed better than the rest of the models. We should attempt better models such as LSTM in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
